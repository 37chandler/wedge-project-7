{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['transArchive_201410_201412.zip',\n",
       " 'transArchive_201301_201303_inactive.zip',\n",
       " 'transArchive_201210_201212.zip',\n",
       " 'transArchive_201609.zip',\n",
       " 'transArchive_201608.zip',\n",
       " 'transArchive_201201_201203.zip',\n",
       " 'transArchive_201204_201206.zip',\n",
       " 'transArchive_201407_201409.zip',\n",
       " 'transArchive_201207_201209.zip',\n",
       " 'transArchive_201404_201406.zip',\n",
       " 'transArchive_201401_201403.zip',\n",
       " 'transArchive_201404_201406_inactive.zip',\n",
       " 'transArchive_201210_201212_inactive.zip',\n",
       " 'transArchive_201307_201309_inactive.zip',\n",
       " 'transArchive_201501_201503.zip',\n",
       " 'transArchive_201307_201309.zip',\n",
       " 'transArchive_201504_201506.zip',\n",
       " 'transArchive_201304_201306.zip',\n",
       " 'transArchive_201507_201509.zip',\n",
       " 'transArchive_201301_201303.zip',\n",
       " 'transArchive_201204_201206_inactive.zip',\n",
       " 'transArchive_201410_201412_inactive.zip',\n",
       " 'transArchive_201310_201312.zip',\n",
       " 'transArchive_201106.zip',\n",
       " 'transArchive_201110_201112.zip',\n",
       " 'transArchive_201701.zip',\n",
       " 'transArchive_201310_201312_inactive.zip',\n",
       " 'transArchive_201105.zip',\n",
       " 'transArchive_201104.zip',\n",
       " 'transArchive_201510.zip',\n",
       " 'transArchive_201201_201203_inactive.zip',\n",
       " 'transArchive_201101_201103.zip',\n",
       " 'transArchive_201107_201109.zip',\n",
       " 'transArchive_201511.zip',\n",
       " 'transArchive_201407_201409_inactive.zip',\n",
       " 'transArchive_201512.zip',\n",
       " 'transArchive_201603.zip',\n",
       " 'transArchive_201304_201306_inactive.zip',\n",
       " 'transArchive_201602.zip',\n",
       " 'transArchive_201007_201009.zip',\n",
       " 'transArchive_201001_201003.zip',\n",
       " 'transArchive_201601.zip',\n",
       " 'transArchive_201004_201006.zip',\n",
       " 'transArchive_201207_201209_inactive.zip',\n",
       " 'transArchive_201605.zip',\n",
       " 'transArchive_201611.zip',\n",
       " 'transArchive_201610.zip',\n",
       " 'transArchive_201604.zip',\n",
       " 'transArchive_201612.zip',\n",
       " 'transArchive_201606.zip',\n",
       " 'transArchive_201010_201012.zip',\n",
       " 'transArchive_201401_201403_inactive.zip',\n",
       " 'transArchive_201607.zip']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"../data/wedgezipofzips/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/juliehilley/Desktop/__ADA Fall 24/_wedge-project/code'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['transArchive_201310_201312_small.zip',\n",
       " 'transArchive_201207_201209_small.zip',\n",
       " 'transArchive_201204_201206_inactive_small.zip',\n",
       " 'transArchive_201304_201306_inactive_small.zip',\n",
       " 'transArchive_201007_201009_small.zip',\n",
       " 'transArchive_201105_small.zip',\n",
       " 'transArchive_201110_201112_small.zip',\n",
       " 'transArchive_201304_201306_small.zip',\n",
       " 'transArchive_201404_201406_inactive_small.zip',\n",
       " 'transArchive_201504_201506_small.zip',\n",
       " 'transArchive_201612_small.zip',\n",
       " 'transArchive_201606_small.zip',\n",
       " 'transArchive_201401_201403_inactive_small.zip',\n",
       " 'transArchive_201407_201409_small.zip',\n",
       " 'transArchive_201201_201203_inactive_small.zip',\n",
       " 'transArchive_201301_201303_inactive_small.zip',\n",
       " 'transArchive_201310_201312_inactive_small.zip',\n",
       " 'transArchive_201107_201109_small.zip',\n",
       " 'transArchive_201601_small.zip',\n",
       " 'transArchive_201210_201212_inactive_small.zip',\n",
       " 'transArchive_201010_201012_small.zip',\n",
       " 'transArchive_201204_201206_small.zip',\n",
       " 'transArchive_201410_201412_inactive_small.zip',\n",
       " 'transArchive_201210_201212_small.zip',\n",
       " 'transArchive_201104_small.zip',\n",
       " 'transArchive_201307_201309_small.zip',\n",
       " 'transArchive_201512_small.zip',\n",
       " 'transArchive_201004_201006_small.zip',\n",
       " 'transArchive_201207_201209_inactive_small.zip',\n",
       " 'transArchive_201307_201309_inactive_small.zip',\n",
       " 'transArchive_201507_201509_small.zip',\n",
       " 'transArchive_201607_small.zip',\n",
       " 'transArchive_201410_201412_small.zip',\n",
       " 'transArchive_201404_201406_small.zip',\n",
       " 'transArchive_201407_201409_inactive_small.zip',\n",
       " 'transArchive_201602_small.zip',\n",
       " 'transArchive_201301_201303_small.zip',\n",
       " 'transArchive_201101_201103_small.zip',\n",
       " 'transArchive_201511_small.zip',\n",
       " 'transArchive_201610_small.zip',\n",
       " 'transArchive_201604_small.zip',\n",
       " 'transArchive_201501_201503_small.zip',\n",
       " 'transArchive_201609_small.zip',\n",
       " 'transArchive_201001_201003_small.zip',\n",
       " 'transArchive_201603_small.zip',\n",
       " 'transArchive_201106_small.zip',\n",
       " 'transArchive_201201_201203_small.zip',\n",
       " 'transArchive_201510_small.zip',\n",
       " 'transArchive_201605_small.zip',\n",
       " 'transArchive_201401_201403_small.zip',\n",
       " 'transArchive_201611_small.zip',\n",
       " 'transArchive_201608_small.zip',\n",
       " 'transArchive_201701_small.zip']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"../data/wedgezipofzips_small/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in directory: ['transArchive_201310_201312_small.zip', 'transArchive_201207_201209_small.zip', 'transArchive_201204_201206_inactive_small.zip', 'transArchive_201304_201306_inactive_small.zip', 'transArchive_201007_201009_small.zip', 'transArchive_201105_small.zip', 'transArchive_201110_201112_small.zip', 'transArchive_201304_201306_small.zip', 'transArchive_201404_201406_inactive_small.zip', 'transArchive_201504_201506_small.zip', 'transArchive_201612_small.zip', 'transArchive_201606_small.zip', 'transArchive_201401_201403_inactive_small.zip', 'transArchive_201407_201409_small.zip', 'transArchive_201201_201203_inactive_small.zip', 'transArchive_201301_201303_inactive_small.zip', 'transArchive_201310_201312_inactive_small.zip', 'transArchive_201107_201109_small.zip', 'transArchive_201601_small.zip', 'transArchive_201210_201212_inactive_small.zip', 'transArchive_201010_201012_small.zip', 'transArchive_201204_201206_small.zip', 'transArchive_201410_201412_inactive_small.zip', 'transArchive_201210_201212_small.zip', 'transArchive_201104_small.zip', 'transArchive_201307_201309_small.zip', 'transArchive_201512_small.zip', 'transArchive_201004_201006_small.zip', 'transArchive_201207_201209_inactive_small.zip', 'transArchive_201307_201309_inactive_small.zip', 'transArchive_201507_201509_small.zip', 'transArchive_201607_small.zip', 'transArchive_201410_201412_small.zip', 'transArchive_201404_201406_small.zip', 'transArchive_201407_201409_inactive_small.zip', 'transArchive_201602_small.zip', 'transArchive_201301_201303_small.zip', 'transArchive_201101_201103_small.zip', 'transArchive_201511_small.zip', 'transArchive_201610_small.zip', 'transArchive_201604_small.zip', 'transArchive_201501_201503_small.zip', 'transArchive_201609_small.zip', 'transArchive_201001_201003_small.zip', 'transArchive_201603_small.zip', 'transArchive_201106_small.zip', 'transArchive_201201_201203_small.zip', 'transArchive_201510_small.zip', 'transArchive_201605_small.zip', 'transArchive_201401_201403_small.zip', 'transArchive_201611_small.zip', 'transArchive_201608_small.zip', 'transArchive_201701_small.zip']\n",
      "Extracted: transArchive_201310_201312_small.zip\n",
      "Extracted: transArchive_201207_201209_small.zip\n",
      "Extracted: transArchive_201204_201206_inactive_small.zip\n",
      "Extracted: transArchive_201304_201306_inactive_small.zip\n",
      "Extracted: transArchive_201007_201009_small.zip\n",
      "Extracted: transArchive_201105_small.zip\n",
      "Extracted: transArchive_201110_201112_small.zip\n",
      "Extracted: transArchive_201304_201306_small.zip\n",
      "Extracted: transArchive_201404_201406_inactive_small.zip\n",
      "Extracted: transArchive_201504_201506_small.zip\n",
      "Extracted: transArchive_201612_small.zip\n",
      "Extracted: transArchive_201606_small.zip\n",
      "Extracted: transArchive_201401_201403_inactive_small.zip\n",
      "Extracted: transArchive_201407_201409_small.zip\n",
      "Extracted: transArchive_201201_201203_inactive_small.zip\n",
      "Extracted: transArchive_201301_201303_inactive_small.zip\n",
      "Extracted: transArchive_201310_201312_inactive_small.zip\n",
      "Extracted: transArchive_201107_201109_small.zip\n",
      "Extracted: transArchive_201601_small.zip\n",
      "Extracted: transArchive_201210_201212_inactive_small.zip\n",
      "Extracted: transArchive_201010_201012_small.zip\n",
      "Extracted: transArchive_201204_201206_small.zip\n",
      "Extracted: transArchive_201410_201412_inactive_small.zip\n",
      "Extracted: transArchive_201210_201212_small.zip\n",
      "Extracted: transArchive_201104_small.zip\n",
      "Extracted: transArchive_201307_201309_small.zip\n",
      "Extracted: transArchive_201512_small.zip\n",
      "Extracted: transArchive_201004_201006_small.zip\n",
      "Extracted: transArchive_201207_201209_inactive_small.zip\n",
      "Extracted: transArchive_201307_201309_inactive_small.zip\n",
      "Extracted: transArchive_201507_201509_small.zip\n",
      "Extracted: transArchive_201607_small.zip\n",
      "Extracted: transArchive_201410_201412_small.zip\n",
      "Extracted: transArchive_201404_201406_small.zip\n",
      "Extracted: transArchive_201407_201409_inactive_small.zip\n",
      "Extracted: transArchive_201602_small.zip\n",
      "Extracted: transArchive_201301_201303_small.zip\n",
      "Extracted: transArchive_201101_201103_small.zip\n",
      "Extracted: transArchive_201511_small.zip\n",
      "Extracted: transArchive_201610_small.zip\n",
      "Extracted: transArchive_201604_small.zip\n",
      "Extracted: transArchive_201501_201503_small.zip\n",
      "Extracted: transArchive_201609_small.zip\n",
      "Extracted: transArchive_201001_201003_small.zip\n",
      "Extracted: transArchive_201603_small.zip\n",
      "Extracted: transArchive_201106_small.zip\n",
      "Extracted: transArchive_201201_201203_small.zip\n",
      "Extracted: transArchive_201510_small.zip\n",
      "Extracted: transArchive_201605_small.zip\n",
      "Extracted: transArchive_201401_201403_small.zip\n",
      "Extracted: transArchive_201611_small.zip\n",
      "Extracted: transArchive_201608_small.zip\n",
      "Extracted: transArchive_201701_small.zip\n",
      "All files extracted successfully.\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "\n",
    "# Path to directory containing zip files\n",
    "zip_file_dir = '../data/wedgezipofzips_small/'\n",
    "output_dir = '../data/extracted_files/'  # Path where you want to extract the files\n",
    "\n",
    "# List all files in the directory to identify the zip files\n",
    "files_in_dir = os.listdir(zip_file_dir)\n",
    "print(\"Files in directory:\", files_in_dir)\n",
    "\n",
    "# Loop through the files and extract each zip file\n",
    "for file_name in files_in_dir:\n",
    "    if file_name.endswith('.zip'):  # Ensure we are only working with zip files\n",
    "        zip_file_path = os.path.join(zip_file_dir, file_name)\n",
    "        \n",
    "        # Extract the zip file\n",
    "        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(output_dir)\n",
    "        \n",
    "        print(f\"Extracted: {file_name}\")\n",
    "\n",
    "print(\"All files extracted successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Exploration : view, check types, look for missing/null values, summary stats, convert datetime and add columns, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              datetime  register_no  emp_no  trans_no            upc  \\\n",
      "0  2012-07-01 08:20:15           16      54         1  0000000009506   \n",
      "1  2012-07-01 08:20:17           16      54         1  0000000009505   \n",
      "2  2012-07-01 08:20:20           16      54         1  0000000009504   \n",
      "3  2012-07-01 08:20:21           16      54         1  0000000009508   \n",
      "4  2012-07-01 08:20:27           16      54         1  0020901500000   \n",
      "\n",
      "                    description trans_type trans_subtype trans_status  \\\n",
      "0      Offsite: Plain Croissant          I                              \n",
      "1  Offsite: Chocolate Croissant          I                              \n",
      "2     Offsite: Almond Croissant          I                              \n",
      "3  Offsite: Cream Cheese Danish          I                              \n",
      "4     Offsite: Blueberry Muffin          I                              \n",
      "\n",
      "   department  ...  batchHeaderID  local  organic  display  receipt  card_no  \\\n",
      "0          17  ...            NaN      0      0.0                 0    50054   \n",
      "1          17  ...            NaN      0      0.0                 0    50054   \n",
      "2          17  ...            NaN      0      0.0                 0    50054   \n",
      "3          17  ...            NaN      0      0.0                 0    50054   \n",
      "4           8  ...            NaN      0      0.0                 0    50054   \n",
      "\n",
      "   store  branch  match_id  trans_id  \n",
      "0     16       0         0         4  \n",
      "1     16       0         0         5  \n",
      "2     16       0         0         6  \n",
      "3     16       0         0         7  \n",
      "4     16       0         0         8  \n",
      "\n",
      "[5 rows x 50 columns]\n",
      "Index(['datetime', 'register_no', 'emp_no', 'trans_no', 'upc', 'description',\n",
      "       'trans_type', 'trans_subtype', 'trans_status', 'department', 'quantity',\n",
      "       'Scale', 'cost', 'unitPrice', 'total', 'regPrice', 'altPrice', 'tax',\n",
      "       'taxexempt', 'foodstamp', 'wicable', 'discount', 'memDiscount',\n",
      "       'discountable', 'discounttype', 'voided', 'percentDiscount', 'ItemQtty',\n",
      "       'volDiscType', 'volume', 'VolSpecial', 'mixMatch', 'matched', 'memType',\n",
      "       'staff', 'numflag', 'itemstatus', 'tenderstatus', 'charflag', 'varflag',\n",
      "       'batchHeaderID', 'local', 'organic', 'display', 'receipt', 'card_no',\n",
      "       'store', 'branch', 'match_id', 'trans_id'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Load a sample file\n",
    "file_name = os.listdir(output_dir)[0]  # Load the first CSV file\n",
    "file_path = os.path.join(output_dir, file_name)\n",
    "\n",
    "# Load a small sample (first 100 rows)\n",
    "df = pd.read_csv(file_path, nrows=100)\n",
    "\n",
    "# Display the first few rows and column names\n",
    "print(df.head())\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datetime            object\n",
      "register_no          int64\n",
      "emp_no               int64\n",
      "trans_no             int64\n",
      "upc                 object\n",
      "description         object\n",
      "trans_type          object\n",
      "trans_subtype       object\n",
      "trans_status        object\n",
      "department           int64\n",
      "quantity           float64\n",
      "Scale                int64\n",
      "cost               float64\n",
      "unitPrice          float64\n",
      "total              float64\n",
      "regPrice           float64\n",
      "altPrice           float64\n",
      "tax                  int64\n",
      "taxexempt            int64\n",
      "foodstamp            int64\n",
      "wicable              int64\n",
      "discount           float64\n",
      "memDiscount        float64\n",
      "discountable         int64\n",
      "discounttype         int64\n",
      "voided               int64\n",
      "percentDiscount    float64\n",
      "ItemQtty           float64\n",
      "volDiscType          int64\n",
      "volume               int64\n",
      "VolSpecial         float64\n",
      "mixMatch             int64\n",
      "matched              int64\n",
      "memType            float64\n",
      "staff              float64\n",
      "numflag              int64\n",
      "itemstatus           int64\n",
      "tenderstatus         int64\n",
      "charflag            object\n",
      "varflag              int64\n",
      "batchHeaderID      float64\n",
      "local                int64\n",
      "organic            float64\n",
      "display             object\n",
      "receipt              int64\n",
      "card_no              int64\n",
      "store                int64\n",
      "branch               int64\n",
      "match_id             int64\n",
      "trans_id             int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datetime             0\n",
      "register_no          0\n",
      "emp_no               0\n",
      "trans_no             0\n",
      "upc                  0\n",
      "description          0\n",
      "trans_type           0\n",
      "trans_subtype        0\n",
      "trans_status         0\n",
      "department           0\n",
      "quantity             0\n",
      "Scale                0\n",
      "cost                 0\n",
      "unitPrice            0\n",
      "total                0\n",
      "regPrice             0\n",
      "altPrice             0\n",
      "tax                  0\n",
      "taxexempt            0\n",
      "foodstamp            0\n",
      "wicable              0\n",
      "discount             0\n",
      "memDiscount          0\n",
      "discountable         0\n",
      "discounttype         0\n",
      "voided               0\n",
      "percentDiscount     81\n",
      "ItemQtty             0\n",
      "volDiscType          0\n",
      "volume               0\n",
      "VolSpecial           0\n",
      "mixMatch             0\n",
      "matched              0\n",
      "memType            100\n",
      "staff              100\n",
      "numflag              0\n",
      "itemstatus           0\n",
      "tenderstatus         0\n",
      "charflag            97\n",
      "varflag              0\n",
      "batchHeaderID      100\n",
      "local                0\n",
      "organic             27\n",
      "display              0\n",
      "receipt              0\n",
      "card_no              0\n",
      "store                0\n",
      "branch               0\n",
      "match_id             0\n",
      "trans_id             0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50054 50052 50056 50048     3 16052 22511]\n",
      "card_no\n",
      "50048    21\n",
      "3        20\n",
      "50054    17\n",
      "50052    17\n",
      "50056    15\n",
      "16052     6\n",
      "22511     4\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check unique values in 'card_no' (owners vs. non-owners)\n",
    "print(df['card_no'].unique())\n",
    "\n",
    "print(df['card_no'].value_counts())\n",
    "\n",
    "non_owner_transactions = df[df['card_no'] == 3]\n",
    "owner_transactions = df[df['card_no'] != 3]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Owner with the most transactions: 50048\n",
      "Number of transactions: 21\n"
     ]
    }
   ],
   "source": [
    "# Group by 'card_no' to count the number of transactions for each owner\n",
    "transaction_counts = df.groupby('card_no').size().reset_index(name='num_transactions')\n",
    "\n",
    "# Find the owner with the most transactions\n",
    "max_transactions_owner = transaction_counts.loc[transaction_counts['num_transactions'].idxmax()]\n",
    "\n",
    "print(f\"Owner with the most transactions: {max_transactions_owner['card_no']}\")\n",
    "print(f\"Number of transactions: {max_transactions_owner['num_transactions']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datetime           datetime64[ns]\n",
      "register_no                 int64\n",
      "emp_no                      int64\n",
      "trans_no                    int64\n",
      "upc                        object\n",
      "description                object\n",
      "trans_type                 object\n",
      "trans_subtype              object\n",
      "trans_status               object\n",
      "department                  int64\n",
      "quantity                  float64\n",
      "Scale                       int64\n",
      "cost                      float64\n",
      "unitPrice                 float64\n",
      "total                     float64\n",
      "regPrice                  float64\n",
      "altPrice                  float64\n",
      "tax                         int64\n",
      "taxexempt                   int64\n",
      "foodstamp                   int64\n",
      "wicable                     int64\n",
      "discount                  float64\n",
      "memDiscount               float64\n",
      "discountable                int64\n",
      "discounttype                int64\n",
      "voided                      int64\n",
      "percentDiscount           float64\n",
      "ItemQtty                  float64\n",
      "volDiscType                 int64\n",
      "volume                      int64\n",
      "VolSpecial                float64\n",
      "mixMatch                    int64\n",
      "matched                     int64\n",
      "memType                   float64\n",
      "staff                     float64\n",
      "numflag                     int64\n",
      "itemstatus                  int64\n",
      "tenderstatus                int64\n",
      "charflag                   object\n",
      "varflag                     int64\n",
      "batchHeaderID             float64\n",
      "local                       int64\n",
      "organic                   float64\n",
      "display                    object\n",
      "receipt                     int64\n",
      "card_no                     int64\n",
      "store                       int64\n",
      "branch                      int64\n",
      "match_id                    int64\n",
      "trans_id                    int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Convert 'datetime' column to datetime format\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "\n",
    "# Check the conversion\n",
    "print(df.dtypes)  # This will show that 'datetime' is now of type datetime64[ns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             datetime        date  hour  year  month\n",
      "0 2012-07-01 08:20:15  2012-07-01     8  2012      7\n",
      "1 2012-07-01 08:20:17  2012-07-01     8  2012      7\n",
      "2 2012-07-01 08:20:20  2012-07-01     8  2012      7\n",
      "3 2012-07-01 08:20:21  2012-07-01     8  2012      7\n",
      "4 2012-07-01 08:20:27  2012-07-01     8  2012      7\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create new columns for date, hour, year, and month\n",
    "df['date'] = df['datetime'].dt.date         # Extract date (YYYY-MM-DD)\n",
    "df['hour'] = df['datetime'].dt.hour         # Extract hour (HH)\n",
    "df['year'] = df['datetime'].dt.year         # Extract year (YYYY)\n",
    "df['month'] = df['datetime'].dt.month       # Extract month (MM)\n",
    "\n",
    "# Check the new columns\n",
    "print(df[['datetime', 'date', 'hour', 'year', 'month']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       register_no      emp_no    trans_no  department    quantity   Scale  \\\n",
      "count   100.000000  100.000000  100.000000  100.000000  100.000000  100.00   \n",
      "mean     13.630000   47.850000    2.940000    8.110000   21.777600    0.01   \n",
      "std       4.624605   19.843517    1.710455    6.801508   92.658068    0.10   \n",
      "min       4.000000    3.000000    1.000000    0.000000   -8.000000    0.00   \n",
      "25%      16.000000   54.000000    2.000000    0.000000    1.000000    0.00   \n",
      "50%      16.000000   54.000000    3.000000    8.000000    2.000000    0.00   \n",
      "75%      16.000000   54.000000    4.000000   17.000000    4.000000    0.00   \n",
      "max      16.000000   79.000000    8.000000   17.000000  750.000000    1.00   \n",
      "\n",
      "             cost   unitPrice       total    regPrice  ...    organic  \\\n",
      "count  100.000000  100.000000  100.000000  100.000000  ...  73.000000   \n",
      "mean     0.304132    1.279100    0.447400    1.326100  ...   0.013699   \n",
      "std      0.554576    1.193773   21.170878    1.288315  ...   0.311500   \n",
      "min      0.000000   -0.100000 -125.890000   -0.100000  ...  -1.000000   \n",
      "25%      0.000000    0.000000    0.000000    0.000000  ...   0.000000   \n",
      "50%      0.000000    1.590000    3.335000    1.590000  ...   0.000000   \n",
      "75%      0.520000    1.705000    6.520000    1.705000  ...   0.000000   \n",
      "max      3.028000    5.990000   41.250000    5.990000  ...   1.000000   \n",
      "\n",
      "       receipt       card_no       store  branch  match_id    trans_id  \\\n",
      "count    100.0    100.000000  100.000000   100.0     100.0  100.000000   \n",
      "mean       0.0  36900.660000    7.440000     0.0       0.0   10.670000   \n",
      "std        0.0  20785.654778   10.051705     0.0       0.0    6.380296   \n",
      "min        0.0      3.000000    1.000000     0.0       0.0    1.000000   \n",
      "25%        0.0  16052.000000    1.000000     0.0       0.0    6.000000   \n",
      "50%        0.0  50048.000000    2.000000     0.0       0.0    9.000000   \n",
      "75%        0.0  50054.000000    8.000000     0.0       0.0   15.000000   \n",
      "max        0.0  50056.000000   32.000000     0.0       0.0   26.000000   \n",
      "\n",
      "             hour    year  month  \n",
      "count  100.000000   100.0  100.0  \n",
      "mean     8.300000  2012.0    7.0  \n",
      "std      0.460566     0.0    0.0  \n",
      "min      8.000000  2012.0    7.0  \n",
      "25%      8.000000  2012.0    7.0  \n",
      "50%      8.000000  2012.0    7.0  \n",
      "75%      9.000000  2012.0    7.0  \n",
      "max      9.000000  2012.0    7.0  \n",
      "\n",
      "[8 rows x 45 columns]\n"
     ]
    }
   ],
   "source": [
    "# Select only the numeric columns\n",
    "numeric_cols = df.select_dtypes(include='number')\n",
    "\n",
    "# Get summary statistics for numeric columns\n",
    "numeric_summary = numeric_cols.describe()\n",
    "\n",
    "# Display the summary stats\n",
    "print(numeric_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             cost   unitPrice       total\n",
      "count  100.000000  100.000000  100.000000\n",
      "mean     0.304132    1.279100    0.447400\n",
      "std      0.554576    1.193773   21.170878\n",
      "min      0.000000   -0.100000 -125.890000\n",
      "25%      0.000000    0.000000    0.000000\n",
      "50%      0.000000    1.590000    3.335000\n",
      "75%      0.520000    1.705000    6.520000\n",
      "max      3.028000    5.990000   41.250000\n"
     ]
    }
   ],
   "source": [
    "# List of columns to include\n",
    "columns_to_include = ['cost', 'unitPrice', 'total']\n",
    "\n",
    "# Select only these columns from the DataFrame\n",
    "selected_numeric_cols = df[columns_to_include]\n",
    "\n",
    "# Get summary statistics for the selected columns\n",
    "numeric_summary = selected_numeric_cols.describe()\n",
    "\n",
    "# Display the summary stats\n",
    "print(numeric_summary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table 1: Sales by Date by Hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sales by Date by Hour table created successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# SQLite database connection\n",
    "conn = sqlite3.connect('sales_summary.db')\n",
    "\n",
    "# Create the table for 'Sales by Date by Hour'\n",
    "conn.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS sales_by_date_by_hour (\n",
    "        sale_date TEXT,\n",
    "        sale_hour INTEGER,\n",
    "        total_sales REAL,\n",
    "        num_transactions INTEGER,\n",
    "        total_items INTEGER\n",
    "    )\n",
    "''')\n",
    "\n",
    "# Group by date and hour to summarize the sales data\n",
    "sales_by_date_by_hour = df.groupby(['date', 'hour']).agg({\n",
    "    'total': 'sum',\n",
    "    'trans_no': 'nunique',\n",
    "    'quantity': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# Rename the columns\n",
    "sales_by_date_by_hour.columns = ['sale_date', 'sale_hour', 'total_sales', 'num_transactions', 'total_items']\n",
    "\n",
    "# Insert the summarized data into SQLite\n",
    "sales_by_date_by_hour.to_sql('sales_by_date_by_hour', conn, if_exists='replace', index=False)\n",
    "\n",
    "print(\"Sales by Date by Hour table created successfully.\")\n",
    "\n",
    "# Commit and close the connection\n",
    "conn.commit()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table 2: Sales by Owner by Year by Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sales by Owner by Year by Month table created successfully.\n"
     ]
    }
   ],
   "source": [
    "# SQLite database connection\n",
    "conn = sqlite3.connect('sales_summary.db')\n",
    "\n",
    "# Create the table for 'Sales by Owner by Year by Month'\n",
    "conn.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS sales_by_owner_by_year_by_month (\n",
    "        card_no INTEGER,\n",
    "        sale_year INTEGER,\n",
    "        sale_month INTEGER,\n",
    "        total_sales REAL,\n",
    "        num_transactions INTEGER,\n",
    "        total_items INTEGER\n",
    "    )\n",
    "''')\n",
    "\n",
    "# Group by card_no, year, and month to summarize the sales data\n",
    "sales_by_owner_by_year_by_month = df.groupby(['card_no', 'year', 'month']).agg({\n",
    "    'total': 'sum',\n",
    "    'trans_no': 'nunique',\n",
    "    'quantity': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# Rename the columns\n",
    "sales_by_owner_by_year_by_month.columns = ['card_no', 'sale_year', 'sale_month', 'total_sales', 'num_transactions', 'total_items']\n",
    "\n",
    "# Insert the summarized data into SQLite\n",
    "sales_by_owner_by_year_by_month.to_sql('sales_by_owner_by_year_by_month', conn, if_exists='replace', index=False)\n",
    "\n",
    "print(\"Sales by Owner by Year by Month table created successfully.\")\n",
    "\n",
    "# Commit and close the connection\n",
    "conn.commit()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table 3: Sales by Product Description by Year by Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sales by Product Description by Year by Month table created successfully.\n"
     ]
    }
   ],
   "source": [
    "# SQLite database connection\n",
    "conn = sqlite3.connect('sales_summary.db')\n",
    "\n",
    "# Create the table for 'Sales by Product Description by Year by Month'\n",
    "conn.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS sales_by_product_description_by_year_by_month (\n",
    "        upc TEXT,\n",
    "        description TEXT,\n",
    "        department_number INTEGER,\n",
    "        sale_year INTEGER,\n",
    "        sale_month INTEGER,\n",
    "        total_sales REAL,\n",
    "        num_transactions INTEGER,\n",
    "        total_items INTEGER\n",
    "    )\n",
    "''')\n",
    "\n",
    "# Group by upc, description, department, year, and month to summarize the sales data\n",
    "sales_by_product_description_by_year_by_month = df.groupby(['upc', 'description', 'department', 'year', 'month']).agg({\n",
    "    'total': 'sum',\n",
    "    'trans_no': 'nunique',\n",
    "    'quantity': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# Rename the columns\n",
    "sales_by_product_description_by_year_by_month.columns = ['upc', 'description', 'department_number', 'sale_year', 'sale_month', 'total_sales', 'num_transactions', 'total_items']\n",
    "\n",
    "# Insert the summarized data into SQLite\n",
    "sales_by_product_description_by_year_by_month.to_sql('sales_by_product_description_by_year_by_month', conn, if_exists='replace', index=False)\n",
    "\n",
    "print(\"Sales by Product Description by Year by Month table created successfully.\")\n",
    "\n",
    "# Commit and close the connection\n",
    "conn.commit()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
